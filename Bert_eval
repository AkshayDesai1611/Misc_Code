from transformers import AutoTokenizer, AutoModel
from scipy import spatial

def sentence_similarity(sentence1, sentence2):
    # Load pre-trained BERT model and tokenizer
    model_name = "bert-base-uncased"
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModel.from_pretrained(model_name)

    # Tokenize and encode sentences
    tokens1 = tokenizer(sentence1, return_tensors="pt")
    tokens2 = tokenizer(sentence2, return_tensors="pt")

    # Get BERT embeddings for [CLS] tokens
    with torch.no_grad():
        output1 = model(**tokens1)
        output2 = model(**tokens2)

    # Use cosine similarity between embeddings
    embedding1 = output1.last_hidden_state.mean(dim=1).squeeze().numpy()
    embedding2 = output2.last_hidden_state.mean(dim=1).squeeze().numpy()
    similarity_score = 1 - spatial.distance.cosine(embedding1, embedding2)

    return similarity_score

# Example usage
sentence1 = "Python is a versatile programming language."
sentence2 = "Programming in Python is flexible."

score = sentence_similarity(sentence1, sentence2)
print(f"Similarity Score: {score}")
